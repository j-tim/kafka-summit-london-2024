server:
  port: 8085
spring:
  application:
    name: producer-application-2

  kafka:
    bootstrap-servers: localhost:9092

    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
      client-id: ${spring.application.name}
      properties:
        auto.register.schemas: false
        avro.remove.java.properties: true
    streams:
      application-id: kafka-streams-example-app
      bootstrap-servers: ${spring.kafka.bootstrap-servers}
      # This setting is here to don't wait until the buffer is full
      properties:
        schema.registry.url: http://schema-registry:8081
        default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
        default.value.serde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
        # LogAndFailExceptionHandler is the default!
        default.deserialization.exception.handler: org.apache.kafka.streams.errors.LogAndFailExceptionHandler
      state-store-cache-max-size: 0
    #        default.deserialization.exception.handler: org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
    #        default.deserialization.exception.handler: org.springframework.kafka.streams.RecoveringDeserializationExceptionHandler

    properties:
      schema.registry.url: http://localhost:8081

# Open up all Spring Boot Actuator endpoints
management:
  endpoints:
    web:
      exposure:
        include: "*"
merak-kafka:
  example:
    streams:
      output:
        topicName: forward-output-topic
    producer:
      topicName: customers-topic-forward
kafka:
  producer:
    enabled: true
    rate: 3000

